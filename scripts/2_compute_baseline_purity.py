"""
Compute the baseline lexical purity metrics for the processed Brown corpus.

Inputs:
    data/processed/brown_<variant>.jsonl plus the accompanying term index and
    term frequency maps generated by scripts/1_preprocessing.py.
Outputs:
    data/results/purity_baseline_<variant>.csv,
    data/results/purity_metadata_<variant>.json, and
    data/visuals/purity_distribution_<variant>.png capturing the corpus stats.
Usage:
    python scripts/2_compute_baseline_purity.py --variant {clean,nostop}
"""

import os
import json
import datetime
import random
import csv
from collections import Counter
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

# ---------------------
# Configuration
# ---------------------
DATA_DIR = "data"
PROCESSED_DIR = os.path.join(DATA_DIR, "processed")
RESULTS_DIR = os.path.join(DATA_DIR, "results")
VISUALS_DIR = os.path.join(DATA_DIR, "visuals")

os.makedirs(RESULTS_DIR, exist_ok=True)
os.makedirs(VISUALS_DIR, exist_ok=True)

SEED = 42
MIN_DOCS_PER_TERM = 5
random.seed(SEED)


def corpus_entropy(docs):
    """Compute Shannon entropy (bits) over token distribution for a list of docs."""
    total = sum(len(d["tokens"]) for d in docs)
    if total == 0:
        return 0.0
    freq = Counter(tok for d in docs for tok in d["tokens"])
    probs = np.array(list(freq.values()), dtype=float) / total
    return -np.sum(probs * np.log2(probs))


def purity_gain(term, term_index, docs, H_C):
    """Return (G_true, H_Ct, H_Cnot, n_docs_t, n_docs_not, H_weighted)."""
    docset_t = set(term_index[term])
    docs_t = [d for d in docs if d["id"] in docset_t]
    docs_nt = [d for d in docs if d["id"] not in docset_t]
    if not docs_t or not docs_nt:
        return 0.0, 0.0, 0.0, 0, 0, 0.0

    P_t = len(docs_t) / len(docs)
    P_nt = 1 - P_t
    H_t = corpus_entropy(docs_t)
    H_nt = corpus_entropy(docs_nt)
    H_weighted = P_t * H_t + P_nt * H_nt
    G = H_C - H_weighted
    return G, H_t, H_nt, len(docs_t), len(docs_nt), H_weighted


def freq_bin(freq):
    """
    Bucket an absolute term frequency into a coarse rarity bin.

    Args:
        freq: Integer count drawn from term_freqs.
    Returns:
        "rare" (<10 tokens), "mid" (<100 tokens), or "common" (otherwise).
    """
    if freq < 10:
        return "rare"
    if freq < 100:
        return "mid"
    return "common"


def run(variant="clean"):
    """
    Execute the purity computation for a chosen preprocessing variant.

    Args:
        variant: Either "clean" (default) or "nostop", matching file suffixes.
    """
    assert variant in ("clean", "nostop"), "variant must be 'clean' or 'nostop'"

    proc_prefix = f"brown_{variant}"
    docs_path = os.path.join(PROCESSED_DIR, f"{proc_prefix}.jsonl")
    idx_path = os.path.join(PROCESSED_DIR, f"term_index_{variant}.json")
    freq_path = os.path.join(PROCESSED_DIR, f"term_freqs_{variant}.json")

    # Load
    with open(docs_path) as f:
        docs = [json.loads(l) for l in f]
    with open(idx_path) as f:
        term_index = json.load(f)
    with open(freq_path) as f:
        term_freqs = json.load(f)

    vocab_size = len(term_index)
    n_docs = len(docs)
    total_tokens = sum(len(d["tokens"]) for d in docs)

    print(f"Loaded {n_docs} docs, vocab {vocab_size}, tokens {total_tokens}")

    # Corpus entropy
    H_C = corpus_entropy(docs)
    print(f"Corpus entropy H(C) = {H_C:.4f} bits")

    # Select eligible terms
    terms = [t for t, ids in term_index.items() if len(ids) >= MIN_DOCS_PER_TERM]
    print(f"Computing purity for {len(terms)} terms (min_docs_per_term={MIN_DOCS_PER_TERM})")

    results = []
    for term in tqdm(terms, desc="terms", ncols=80):
        G, H_t, H_nt, n_docs_t, n_docs_nt, H_weighted = purity_gain(term, term_index, docs, H_C)
        freq_total = int(term_freqs.get(term, 0))
        results.append({
            "term": term,
            "G_true": float(G),
            "G_frac": float(G / H_C) if H_C > 0 else 0.0,
            "H_weighted": float(H_weighted),
            "H_Ct": float(H_t),
            "H_Cnot": float(H_nt),
            "freq_total": freq_total,
            "freq_bin": freq_bin(freq_total),
            "n_docs_total": len(term_index[term]),
            "n_docs_t": n_docs_t,
            "n_docs_not": n_docs_nt,
        })

    # Save CSV
    out_csv = os.path.join(RESULTS_DIR, f"purity_baseline_{variant}.csv")
    with open(out_csv, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=list(results[0].keys()))
        writer.writeheader()
        writer.writerows(results)

    # Save metadata
    meta = {
        "date": datetime.datetime.now().isoformat(),
        "variant": variant,
        "seed": SEED,
        "min_docs_per_term": MIN_DOCS_PER_TERM,
        "H_C": H_C,
        "vocab_size": vocab_size,
        "n_docs": n_docs,
        "total_tokens": total_tokens,
    }
    meta_path = os.path.join(RESULTS_DIR, f"purity_metadata_{variant}.json")
    with open(meta_path, "w") as f:
        json.dump(meta, f, indent=2)

    # Plot
    plt.figure(figsize=(8, 5))
    plt.hist([r["G_true"] for r in results], bins=60, color="C0", alpha=0.8)
    plt.xlabel("G_true(t)")
    plt.ylabel("Number of terms")
    plt.title(f"Lexical purity distribution ({variant})")
    plt.tight_layout()
    out_img = os.path.join(VISUALS_DIR, f"purity_distribution_{variant}.png")
    plt.savefig(out_img, dpi=200)
    plt.close()

    print("Done.")
    print(f"Results: {out_csv}")
    print(f"Metadata: {meta_path}")
    print(f"Plot: {out_img}")


if __name__ == "__main__":
    # default to clean variant; change to 'nostop' to include stopwords
    run(variant="clean")
